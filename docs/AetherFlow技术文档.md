# AetherFlow - æ™ºèƒ½æµå¼æ•°æ®å¤„ç†æ¡†æ¶æŠ€æœ¯æ–‡æ¡£

## æ¦‚è¿°

AetherFlow æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ Python æ•°æ®æµå¤„ç†æ¡†æ¶ï¼Œä¸“ä¸ºæ„å»ºå¯æ‰©å±•ã€çº¿ç¨‹å®‰å…¨çš„æ•°æ®å¤„ç†ç®¡é“è€Œè®¾è®¡ã€‚é€šè¿‡æµå¼æ¥å£ï¼ˆFluent Interfaceï¼‰å’Œæ™ºèƒ½ä¾èµ–æ³¨å…¥ç³»ç»Ÿï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿä»¥å£°æ˜å¼çš„æ–¹å¼æ„å»ºå¤æ‚çš„æ•°æ®å¤„ç†å·¥ä½œæµã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”— **å£°æ˜å¼æµç¨‹å®šä¹‰**: é€šè¿‡é“¾å¼ API æ„å»ºæ¸…æ™°çš„æ•°æ®æµ
- ğŸ§µ **çº¿ç¨‹å®‰å…¨**: åŸºäº ThreadLocalSingleton çš„çŠ¶æ€éš”ç¦»æœºåˆ¶
- ğŸ’‰ **æ™ºèƒ½ä¾èµ–æ³¨å…¥**: é›†æˆ dependency-injector çš„ DI ç³»ç»Ÿ
- âš¡ **å¹¶è¡Œå¤„ç†**: æ”¯æŒæ‰‡å‡º/æ‰‡å…¥æ¨¡å¼çš„å¹¶è¡Œå·¥ä½œæµ
- ğŸ”„ **è‡ªåŠ¨é‡è¯•**: å¯é…ç½®çš„é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸å¤„ç†
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£å’Œ Pydantic éªŒè¯

### ç¯å¢ƒè¦æ±‚

- Python 3.10+
- dependency-injector >= 4.48.1
- pydantic >= 2.11.7

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install dependency-injector pydantic
```

### ç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
from aetherflow import node

@node
def load_data(filename):
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    with open(filename, 'r') as f:
        data = f.read().strip().split('\n')
    return {'data': data, 'count': len(data)}

@node
def filter_data(load_data, min_length=3):
    """è¿‡æ»¤æ•°æ®"""
    data = load_data['data']
    filtered = [item for item in data if len(item) >= min_length]
    return {
        'filtered_data': filtered,
        'original_count': load_data['count'],
        'filtered_count': len(filtered)
    }

@node
def save_results(filter_data, output_file):
    """ä¿å­˜ç»“æœ"""
    with open(output_file, 'w') as f:
        f.write('\n'.join(filter_data['filtered_data']))
    return {
        'saved_file': output_file,
        'processed_items': filter_data['filtered_count'],
        'success': True
    }

# æ„å»ºå’Œæ‰§è¡Œç®¡é“
pipeline = load_data.then(filter_data).then(save_results)
result = pipeline.run({
    'filename': 'input.txt',
    'min_length': 3,
    'output_file': 'output.txt'
})
```

## æ ¸å¿ƒæ¦‚å¿µ

### 1. èŠ‚ç‚¹ (Node)

èŠ‚ç‚¹æ˜¯ AetherFlow çš„åŸºæœ¬æ‰§è¡Œå•å…ƒï¼Œé€šè¿‡ [`@node`](src/aetherflow/__init__.py:699) è£…é¥°å™¨å°†æ™®é€šå‡½æ•°è½¬æ¢ä¸ºå¯é“¾æ¥çš„å¤„ç†èŠ‚ç‚¹ã€‚

```python
@node
def process_data(data: dict) -> dict:
    """å¤„ç†æ•°æ®çš„èŠ‚ç‚¹"""
    result = data['input'] * 2
    return {'output': result}
```

### 2. æµå¼æ¥å£ (Fluent Interface)

é€šè¿‡æ–¹æ³•é“¾æ„å»ºæ•°æ®å¤„ç†ç®¡é“ï¼š

| æ–¹æ³• | åŠŸèƒ½ | ç¤ºä¾‹ |
|------|------|------|
| [`.then()`](src/aetherflow/__init__.py:302) | é¡ºåºæ‰§è¡Œ | `node1.then(node2)` |
| [`.fan_out_to()`](src/aetherflow/__init__.py:306) | å¹¶è¡Œæ‰‡å‡º | `source.fan_out_to([task1, task2])` |
| [`.fan_in()`](src/aetherflow/__init__.py:315) | ç»“æœæ±‡å…¥ | `parallel_nodes.fan_in(aggregator)` |
| [`.branch_on()`](src/aetherflow/__init__.py:329) | æ¡ä»¶åˆ†æ”¯ | `condition.branch_on({True: path_a})` |
| [`.repeat()`](src/aetherflow/__init__.py:333) | é‡å¤æ‰§è¡Œ | `processor.repeat(3)` |

### 3. ä¾èµ–æ³¨å…¥

AetherFlow é›†æˆäº† [`BaseFlowContext`](src/aetherflow/__init__.py:230) æä¾›çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†ï¼š

```python
from aetherflow import BaseFlowContext
from dependency_injector.wiring import Provide

@node
def stateful_processor(data, state: dict = Provide[BaseFlowContext.state]):
    """å¸¦çŠ¶æ€çš„å¤„ç†èŠ‚ç‚¹"""
    state['processed_count'] = state.get('processed_count', 0) + 1
    result = data['value'] * 2
    return {'result': result, 'count': state['processed_count']}

# é…ç½®ä¾èµ–æ³¨å…¥
container = BaseFlowContext()
container.wire(modules=[__name__])
```

## æ ¸å¿ƒ API å‚è€ƒ

### [`@node` è£…é¥°å™¨](src/aetherflow/__init__.py:699)

å°†å‡½æ•°è½¬æ¢ä¸º Node å®ä¾‹ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶å’Œä¾èµ–æ³¨å…¥ã€‚

```python
@node(
    name=None,                    # èŠ‚ç‚¹åç§°ï¼Œç”¨äºè°ƒè¯•
    retry_count=3,               # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay=1.0,             # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    exception_types=(Exception,), # éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
    backoff_factor=1.0,          # é€€é¿å› å­
    max_delay=60.0,              # æœ€å¤§é‡è¯•å»¶è¿Ÿ
    enable_retry=True            # æ˜¯å¦å¯ç”¨é‡è¯•
)
def my_function(data):
    pass
```

### [`Node` ç±»](src/aetherflow/__init__.py:240)

èŠ‚ç‚¹çš„æ ¸å¿ƒå®ç°ï¼Œæ”¯æŒå„ç§ç»„åˆæ¨¡å¼ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**

- [`then(next_node)`](src/aetherflow/__init__.py:302): é¡ºåºé“¾æ¥èŠ‚ç‚¹
- [`fan_out_to(nodes, executor="thread")`](src/aetherflow/__init__.py:306): å¹¶è¡Œåˆ†å‘åˆ°å¤šä¸ªèŠ‚ç‚¹
- [`fan_in(aggregator)`](src/aetherflow/__init__.py:315): èšåˆå¹¶è¡Œç»“æœ
- [`branch_on(conditions)`](src/aetherflow/__init__.py:329): æ¡ä»¶åˆ†æ”¯
- [`repeat(times, stop_on_error=False)`](src/aetherflow/__init__.py:333): é‡å¤æ‰§è¡Œ

### [`BaseFlowContext` ç±»](src/aetherflow/__init__.py:230)

ä¾èµ–æ³¨å…¥å®¹å™¨ï¼Œæä¾›çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†ã€‚

```python
class BaseFlowContext(containers.DeclarativeContainer):
    state = providers.ThreadLocalSingleton(dict)        # çº¿ç¨‹æœ¬åœ°çŠ¶æ€
    context = providers.ThreadLocalSingleton(dict)      # çº¿ç¨‹æœ¬åœ°ä¸Šä¸‹æ–‡
    shared_data = providers.Singleton(dict)             # å…¨å±€å…±äº«æ•°æ®
```

### å¼‚å¸¸ç±»å‹

AetherFlow æä¾›å®Œæ•´çš„å¼‚å¸¸ä½“ç³»ï¼š

- [`AetherFlowException`](src/aetherflow/__init__.py:31): åŸºç¡€å¼‚å¸¸ç±»
- [`NodeExecutionException`](src/aetherflow/__init__.py:40): èŠ‚ç‚¹æ‰§è¡Œå¼‚å¸¸
- [`NodeRetryExhaustedException`](src/aetherflow/__init__.py:68): é‡è¯•è€—å°½å¼‚å¸¸
- [`NodeTimeoutException`](src/aetherflow/__init__.py:54): è¶…æ—¶å¼‚å¸¸

## é«˜çº§åŠŸèƒ½

### 1. å¹¶è¡Œå¤„ç†

#### æ‰‡å‡º/æ‰‡å…¥æ¨¡å¼

```python
@node
def data_source():
    return {'numbers': list(range(100))}

@node
def calculate_sum(data):
    return {'sum': sum(data['numbers'])}

@node
def calculate_average(data):
    numbers = data['numbers']
    return {'average': sum(numbers) / len(numbers)}

@node
def combine_results(parallel_results):
    """èšåˆå¹¶è¡Œå¤„ç†ç»“æœ"""
    sum_result = parallel_results['calculate_sum']['sum']
    avg_result = parallel_results['calculate_average']['average']
    return {'sum': sum_result, 'average': avg_result}

# æ„å»ºå¹¶è¡Œç®¡é“
pipeline = (data_source
    .fan_out_to([calculate_sum, calculate_average])
    .fan_in(combine_results))

result = pipeline.run({})
```

#### æ‰§è¡Œå™¨é…ç½®

```python
# çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼ˆé€‚åˆ I/O å¯†é›†å‹ï¼‰
thread_pipeline = source.fan_out_to(
    [task1, task2, task3],
    executor="thread",
    max_workers=4
)

# è¿›ç¨‹æ± æ‰§è¡Œå™¨ï¼ˆé€‚åˆ CPU å¯†é›†å‹ï¼‰
process_pipeline = source.fan_out_to(
    [task1, task2, task3],
    executor="process",
    max_workers=2
)
```

### 2. é‡è¯•æœºåˆ¶

#### åŸºæœ¬é‡è¯•é…ç½®

```python
@node(
    retry_count=5,
    retry_delay=2.0,
    backoff_factor=2.0,       # æŒ‡æ•°é€€é¿
    max_delay=30.0,
    exception_types=(ValueError, ConnectionError)
)
def network_request(data):
    """ç½‘ç»œè¯·æ±‚èŠ‚ç‚¹"""
    import requests
    response = requests.get(data['url'])
    return {'data': response.json()}
```

#### [`RetryConfig` ç±»](src/aetherflow/__init__.py:105)

```python
from aetherflow import RetryConfig

config = RetryConfig(
    retry_count=3,
    retry_delay=1.0,
    exception_types=(ValueError,),
    backoff_factor=2.0,
    max_delay=60.0
)

# æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•ç‰¹å®šå¼‚å¸¸
should_retry = config.should_retry(ValueError("test"))

# è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæ”¯æŒæŒ‡æ•°é€€é¿ï¼‰
delay = config.get_delay(attempt_number)
```

### 3. çŠ¶æ€ç®¡ç†

#### çº¿ç¨‹éš”ç¦»æ¨¡å¼ï¼ˆæ¨èï¼‰

```python
from dependency_injector import providers

class IsolatedContext(BaseFlowContext):
    """æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹çŠ¶æ€"""
    state = providers.ThreadLocalSingleton(dict)
    service = providers.ThreadLocalSingleton(MyService)
```

#### å…±äº«çŠ¶æ€æ¨¡å¼

```python
import threading

class SharedStateService:
    def __init__(self):
        self.counter = 0
        self.lock = threading.Lock()

    def increment(self):
        with self.lock:
            self.counter += 1
            return self.counter

class SharedContext(BaseFlowContext):
    """çº¿ç¨‹é—´åè°ƒ"""
    shared_service = providers.Singleton(SharedStateService)
```

### 4. æ¡ä»¶åˆ†æ”¯

```python
@node
def check_condition(data):
    return data['value'] > 10

@node
def high_value_processor(data):
    return {'result': 'high', 'value': data['value']}

@node
def low_value_processor(data):
    return {'result': 'low', 'value': data['value']}

# æ¡ä»¶åˆ†æ”¯ç®¡é“
pipeline = check_condition.branch_on({
    True: high_value_processor,
    False: low_value_processor
})
```

### 5. å¾ªç¯å¤„ç†

```python
@node
def increment_processor(data):
    current = data.get('value', 0)
    return {'value': current + 1}

# é‡å¤æ‰§è¡Œ 5 æ¬¡
repeated_pipeline = increment_processor.repeat(5)
result = repeated_pipeline.run({'value': 0})  # {'value': 5}

# é‡åˆ°é”™è¯¯åœæ­¢
safe_pipeline = increment_processor.repeat(3, stop_on_error=True)
```

## å¹¶è¡Œç»“æœæ¨¡å‹

[`ParallelResult`](src/aetherflow/__init__.py:16) æ•°æ®ç±»ç”¨äºå°è£…å¹¶è¡Œæ‰§è¡Œçš„ç»“æœï¼š

```python
@dataclass
class ParallelResult:
    node_name: str                # èŠ‚ç‚¹åç§°
    success: bool                 # æ‰§è¡Œæ˜¯å¦æˆåŠŸ
    result: Any = None           # æ‰§è¡Œç»“æœ
    error: str | None = None     # é”™è¯¯ä¿¡æ¯
    error_traceback: str | None = None  # é”™è¯¯å †æ ˆ
    execution_time: float | None = None # æ‰§è¡Œæ—¶é—´
```

å¹¶è¡Œæ‰§è¡Œçš„è¿”å›æ ¼å¼ï¼š

```python
{
    "node_name": {
        "node_name": "èŠ‚ç‚¹åç§°",
        "success": True/False,
        "result": "æ‰§è¡Œç»“æœæˆ–None",
        "error": "é”™è¯¯ä¿¡æ¯æˆ–None",
        "error_traceback": "é”™è¯¯å †æ ˆæˆ–None",
        "execution_time": "æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"
    }
}
```

## æœ€ä½³å®è·µ

### 1. èŠ‚ç‚¹è®¾è®¡åŸåˆ™

- **å•ä¸€èŒè´£**: æ¯ä¸ªèŠ‚ç‚¹åªè´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„å¤„ç†ä»»åŠ¡
- **çº¯å‡½æ•°**: å°½é‡é¿å…å‰¯ä½œç”¨ï¼Œä¾¿äºæµ‹è¯•å’Œè°ƒè¯•
- **æ˜ç¡®æ¥å£**: ä½¿ç”¨ç±»å‹æ³¨è§£å®šä¹‰æ¸…æ™°çš„è¾“å…¥è¾“å‡º

```python
@node
def clean_text(data: dict) -> dict:
    """æ¸…ç†æ–‡æœ¬æ•°æ®"""
    text = data['text'].strip().lower()
    words = text.split()
    cleaned_words = [word for word in words if word.isalpha()]
    return {
        'original_text': data['text'],
        'cleaned_text': ' '.join(cleaned_words),
        'word_count': len(cleaned_words)
    }
```

### 2. é”™è¯¯å¤„ç†ç­–ç•¥

```python
@node(
    retry_count=3,
    retry_delay=1.0,
    exception_types=(requests.RequestException,),
    enable_retry=True
)
def robust_api_call(data):
    """å¥å£®çš„ API è°ƒç”¨"""
    try:
        response = requests.get(data['url'], timeout=10)
        response.raise_for_status()
        return {'success': True, 'data': response.json()}
    except requests.Timeout:
        raise NodeTimeoutException("APIè°ƒç”¨è¶…æ—¶", timeout_seconds=10)
    except requests.RequestException as e:
        raise NodeExecutionException("APIè°ƒç”¨å¤±è´¥", original_exception=e)
```

### 3. çŠ¶æ€ä½¿ç”¨æŒ‡å¯¼

```python
@node
def process_with_state(data, state: dict = Provide[BaseFlowContext.state]):
    """æ­£ç¡®ä½¿ç”¨çŠ¶æ€çš„ç¤ºä¾‹"""
    # è¯»å–çŠ¶æ€
    processed_count = state.get('processed_count', 0)

    # å¤„ç†æ•°æ®
    result = process_data(data)

    # æ›´æ–°çŠ¶æ€
    state['processed_count'] = processed_count + 1
    state['last_result'] = result

    return result
```

### 4. å¹¶å‘æ¨¡å¼é€‰æ‹©

**é€‰æ‹©çº¿ç¨‹éš”ç¦»æ¨¡å¼çš„åœºæ™¯ï¼š**
- ç‹¬ç«‹ä»»åŠ¡å¤„ç†
- ç®€å•çš„å¹¶å‘éœ€æ±‚
- æ–°æ‰‹å¼€å‘è€…
- é«˜å¹¶å‘åœºæ™¯

**é€‰æ‹©å…±äº«çŠ¶æ€æ¨¡å¼çš„åœºæ™¯ï¼š**
- éœ€è¦çº¿ç¨‹é—´åè°ƒ
- å†…å­˜ä½¿ç”¨æ•æ„Ÿ
- å¤æ‚çš„çŠ¶æ€å…±äº«éœ€æ±‚

## ä½¿ç”¨åœºæ™¯

### 1. ETL æ•°æ®å¤„ç†

```python
etl_pipeline = (extract_from_database
    .then(transform_data)
    .then(validate_data)
    .then(load_to_warehouse))
```

### 2. æœºå™¨å­¦ä¹ æ¨ç†

```python
ml_pipeline = (preprocess_data
    .fan_out_to([model_a, model_b, model_c])
    .fan_in(ensemble_predictions)
    .then(postprocess_results))
```

### 3. å®æ—¶æ•°æ®å¤„ç†

```python
realtime_pipeline = (receive_events
    .fan_out_to([fraud_detection, sentiment_analysis])
    .fan_in(generate_alerts)
    .then(send_notifications))
```

### 4. æ‰¹é‡æ–‡ä»¶å¤„ç†

```python
batch_pipeline = (scan_directory
    .fan_out_to([process_images, extract_metadata])
    .fan_in(combine_results)
    .then(save_manifest))
```

## æ€§èƒ½å’Œç›‘æ§

### æ€§èƒ½ç‰¹ç‚¹

- çº¿ç¨‹éš”ç¦»æ¨¡å¼åœ¨é«˜å¹¶å‘ä¸‹æ€§èƒ½ä¼˜åŠ¿æ˜æ˜¾ï¼ˆ+38% @ 16çº¿ç¨‹ï¼‰
- æ™ºèƒ½é‡è¯•æœºåˆ¶å‡å°‘ä¸´æ—¶æ•…éšœå½±å“
- ç±»å‹éªŒè¯å¼€é”€æœ€å°åŒ–

### è°ƒè¯•å’Œæ—¥å¿—

```python
import logging

# å¯ç”¨ AetherFlow æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('aetherflow')

@node(name="debug_processor")  # æŒ‡å®šèŠ‚ç‚¹åç§°ä¾¿äºè°ƒè¯•
def debug_node(data):
    logger.info(f"å¤„ç†æ•°æ®: {data}")
    return process_data(data)
```

### ç›‘æ§å»ºè®®

- ä½¿ç”¨ `ParallelResult.execution_time` ç›‘æ§èŠ‚ç‚¹æ€§èƒ½
- é€šè¿‡çŠ¶æ€è®°å½•å…³é”®æŒ‡æ ‡
- åˆ©ç”¨å¼‚å¸¸ä¿¡æ¯è¿›è¡Œé—®é¢˜è¯Šæ–­

## æ€»ç»“

AetherFlow æä¾›äº†ä¸€ä¸ªå¼ºå¤§è€Œçµæ´»çš„æ•°æ®æµå¤„ç†æ¡†æ¶ï¼Œé€šè¿‡å£°æ˜å¼çš„ API å’Œæ™ºèƒ½çš„çŠ¶æ€ç®¡ç†ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿæ„å»ºä»ç®€å•åˆ°å¤æ‚çš„å„ç§æ•°æ®å¤„ç†ç®¡é“ã€‚å…³é”®ä¼˜åŠ¿åŒ…æ‹¬ï¼š

- **æ˜“ç”¨æ€§**: æµå¼æ¥å£è®©ä»£ç æ¸…æ™°æ˜“æ‡‚
- **å¯é æ€§**: å®Œå–„çš„é‡è¯•å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶
- **æ€§èƒ½**: çº¿ç¨‹å®‰å…¨çš„å¹¶å‘å¤„ç†èƒ½åŠ›
- **æ‰©å±•æ€§**: çµæ´»çš„ä¾èµ–æ³¨å…¥å’ŒçŠ¶æ€ç®¡ç†
- **ç±»å‹å®‰å…¨**: å®Œæ•´çš„ç±»å‹æ³¨è§£æ”¯æŒ

æ— è®ºæ˜¯ç®€å•çš„æ•°æ®è½¬æ¢è¿˜æ˜¯å¤æ‚çš„å¹¶è¡Œå¤„ç†å·¥ä½œæµï¼ŒAetherFlow éƒ½èƒ½æä¾›æ¸…æ™°ã€å¯ç»´æŠ¤çš„è§£å†³æ–¹æ¡ˆã€‚
