# AetherFlow å¿«é€Ÿå…¥é—¨æŒ‡å—

## ä»€ä¹ˆæ˜¯ AetherFlowï¼Ÿ

AetherFlow æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ Python æ•°æ®æµå¤„ç†æ¡†æ¶ï¼Œä¸“ä¸ºæ„å»ºå¯æ‰©å±•ã€å¯ç»´æŠ¤çš„æ•°æ®å¤„ç†ç®¡é“è€Œè®¾è®¡ã€‚å®ƒé‡‡ç”¨**èŠ‚ç‚¹å¼ç¼–ç¨‹æ¨¡å‹**ï¼Œè®©æ‚¨å¯ä»¥å°†å¤æ‚çš„ä¸šåŠ¡é€»è¾‘åˆ†è§£ä¸ºç®€å•ã€å¯é‡ç”¨çš„å¤„ç†å•å…ƒã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”— å£°æ˜å¼æµç¨‹å®šä¹‰**: é€šè¿‡é“¾å¼APIæ„å»ºæ¸…æ™°çš„æ•°æ®æµ
- **ğŸ§µ å†…ç½®å¹¶å‘æ”¯æŒ**: æ”¯æŒçº¿ç¨‹éš”ç¦»å’Œå…±äº«çŠ¶æ€ä¸¤ç§å¹¶å‘æ¨¡å¼
- **ğŸ’‰ ä¾èµ–æ³¨å…¥ç³»ç»Ÿ**: åŸºäºdependency_injectorçš„å¼ºå¤§DIæ”¯æŒ
- **ğŸš€ é«˜æ€§èƒ½æ‰§è¡Œ**: ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œå’Œèµ„æºç®¡ç†
- **ğŸ›¡ï¸ çº¿ç¨‹å®‰å…¨**: é»˜è®¤æä¾›çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€ç®¡ç†
- **ğŸ¯ ç±»å‹æç¤ºå‹å¥½**: å®Œæ•´çš„ç±»å‹æ³¨è§£æ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install aetherflow
```

### ç¬¬ä¸€ä¸ªç¤ºä¾‹

```python
from aetherflow import node

# å®šä¹‰å¤„ç†èŠ‚ç‚¹
@node
def load_data(filename):
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    with open(filename, 'r') as f:
        data = f.read().strip().split('\n')
    return {'data': data, 'count': len(data)}

@node
def filter_data(load_data, min_length=3):
    """è¿‡æ»¤æ•°æ®"""
    data = load_data['data']
    filtered = [item for item in data if len(item) >= min_length]
    return {'filtered_data': filtered, 'original_count': load_data['count'], 'filtered_count': len(filtered)}

@node
def save_results(filter_data, output_file):
    """ä¿å­˜ç»“æœ"""
    with open(output_file, 'w') as f:
        f.write('\n'.join(filter_data['filtered_data']))
    
    return {
        'saved_file': output_file,
        'processed_items': filter_data['filtered_count'],
        'success': True
    }

# æ„å»ºæ•°æ®å¤„ç†ç®¡é“
pipeline = (load_data
            .then(filter_data)
            .then(save_results))

# æ‰§è¡Œç®¡é“
result = pipeline.run({
    'filename': 'input.txt',
    'min_length': 3,
    'output_file': 'output.txt'
})

print(f"å¤„ç†å®Œæˆ: {result}")
```

## ğŸ“‹ åŸºæœ¬æ¦‚å¿µ

### 1. èŠ‚ç‚¹ (Node)

èŠ‚ç‚¹æ˜¯AetherFlowçš„åŸºæœ¬å¤„ç†å•å…ƒï¼Œä½¿ç”¨`@node`è£…é¥°å™¨å®šä¹‰ï¼š

```python
@node
def my_processor(input_data, param1, param2=None):
    """
    èŠ‚ç‚¹å‡½æ•°è¯´æ˜ï¼š
    - ç¬¬ä¸€ä¸ªå‚æ•°é€šå¸¸æ˜¯è¾“å…¥æ•°æ®
    - å¯ä»¥æœ‰å¤šä¸ªå‚æ•°
    - æ”¯æŒé»˜è®¤å€¼
    - è¿”å›å­—å…¸ä½œä¸ºè¾“å‡º
    """
    result = process(input_data, param1, param2)
    return {'processed': result}
```

### 2. æ•°æ®æµ (Data Flow)

èŠ‚ç‚¹å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿æ¥å½¢æˆæ•°æ®æµï¼š

```python
# é¡ºåºæ‰§è¡Œ
flow1 = node_a.then(node_b).then(node_c)

# å¹¶è¡Œæ‰‡å‡º
flow2 = source.fan_out_to([processor_a, processor_b, processor_c])

# æ‰‡å…¥èšåˆ
flow3 = source.fan_out_to([proc_a, proc_b]).fan_in(aggregator)

# æ¡ä»¶åˆ†æ”¯
flow4 = source.branch({
    'condition_a': processor_a,
    'condition_b': processor_b,
    'default': default_processor
})
```

### 3. ä¸Šä¸‹æ–‡ (Context)

ä¸Šä¸‹æ–‡æä¾›ä¾èµ–æ³¨å…¥å’ŒçŠ¶æ€ç®¡ç†ï¼š

```python
from aetherflow import BaseFlowContext
from dependency_injector import providers

class MyContext(BaseFlowContext):
    """è‡ªå®šä¹‰ä¸Šä¸‹æ–‡"""
    # çº¿ç¨‹æœ¬åœ°çŠ¶æ€ (æ¨è)
    state = providers.ThreadLocalSingleton(dict)
    
    # è‡ªå®šä¹‰æœåŠ¡
    database = providers.ThreadLocalSingleton(DatabaseService)
    cache = providers.ThreadLocalSingleton(CacheService)

@node
def data_processor(input_data, database: DatabaseService, cache: CacheService):
    """ä½¿ç”¨æ³¨å…¥çš„æœåŠ¡"""
    # æ£€æŸ¥ç¼“å­˜
    if cache.has(input_data['key']):
        return cache.get(input_data['key'])
    
    # ä»æ•°æ®åº“è·å–
    result = database.query(input_data['query'])
    cache.set(input_data['key'], result)
    
    return result

# ä½¿ç”¨è‡ªå®šä¹‰ä¸Šä¸‹æ–‡
context = MyContext()
result = data_processor.run({'key': 'user:123', 'query': 'SELECT * FROM users'}, context)
```

## ğŸ›¡ï¸ å¹¶å‘æ¨¡å¼é€‰æ‹©

AetherFlowæ”¯æŒä¸¤ç§å¹¶å‘æ¨¡å¼ï¼Œæ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©ï¼š

### æ¨¡å¼1ï¼šçº¿ç¨‹éš”ç¦» (æ¨èæ–°æ‰‹)

```python
class IsolatedContext(BaseFlowContext):
    """çº¿ç¨‹éš”ç¦»ä¸Šä¸‹æ–‡ - æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹çŠ¶æ€"""
    state = providers.ThreadLocalSingleton(dict)
    service = providers.ThreadLocalSingleton(MyService)

# ç‰¹ç‚¹ï¼š
# âœ… çº¿ç¨‹å®‰å…¨ï¼Œæ— ç«äº‰æ¡ä»¶
# âœ… è°ƒè¯•ç®€å•
# âœ… é€‚åˆç‹¬ç«‹ä»»åŠ¡å¤„ç†
# âŒ å†…å­˜ä½¿ç”¨è¾ƒé«˜
```

### æ¨¡å¼2ï¼šå…±äº«çŠ¶æ€ (é€‚åˆé«˜çº§ç”¨æˆ·)

```python
class SharedStateService:
    def __init__(self):
        self.counter = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.counter += 1
            return self.counter

class SharedContext(BaseFlowContext):
    """å…±äº«çŠ¶æ€ä¸Šä¸‹æ–‡ - éœ€è¦æ‰‹åŠ¨åŒæ­¥"""
    shared_service = providers.Singleton(SharedStateService)

# ç‰¹ç‚¹ï¼š
# âœ… å†…å­˜æ•ˆç‡é«˜
# âœ… æ”¯æŒçº¿ç¨‹é—´åè°ƒ
# âŒ éœ€è¦å¹¶å‘ç¼–ç¨‹ç»éªŒ
# âŒ æ½œåœ¨ç«äº‰æ¡ä»¶
```

**å¦‚ä½•é€‰æ‹©ï¼Ÿ**
- ğŸ¯ **ç‹¬ç«‹ä»»åŠ¡å¤„ç†** â†’ é€‰æ‹©çº¿ç¨‹éš”ç¦»æ¨¡å¼
- ğŸ¯ **éœ€è¦çº¿ç¨‹åè°ƒ** â†’ é€‰æ‹©å…±äº«çŠ¶æ€æ¨¡å¼
- ğŸ¯ **æ–°æ‰‹å…¥é—¨** â†’ æ¨èçº¿ç¨‹éš”ç¦»æ¨¡å¼
- ğŸ¯ **å†…å­˜æ•æ„Ÿ** â†’ è€ƒè™‘å…±äº«çŠ¶æ€æ¨¡å¼

## ğŸ’¡ å®ç”¨æ¨¡å¼

### 1. ETL æ•°æ®å¤„ç†

```python
@node
def extract(source_config):
    """æå–æ•°æ®"""
    data = load_from_source(source_config)
    return {'raw_data': data, 'record_count': len(data)}

@node  
def transform(extract, transformation_rules):
    """è½¬æ¢æ•°æ®"""
    transformed = apply_transformations(extract['raw_data'], transformation_rules)
    return {'transformed_data': transformed}

@node
def load(transform, target_config):
    """åŠ è½½æ•°æ®"""
    save_to_target(transform['transformed_data'], target_config)
    return {'loaded_records': len(transform['transformed_data']), 'success': True}

# ETLç®¡é“
etl_pipeline = extract.then(transform).then(load)
```

### 2. æ‰¹å¤„ç†ä»»åŠ¡

```python
@node
def batch_processor(data_batch, batch_size=100):
    """æ‰¹å¤„ç†æ•°æ®"""
    results = []
    for i in range(0, len(data_batch['items']), batch_size):
        batch = data_batch['items'][i:i + batch_size]
        batch_result = process_batch(batch)
        results.extend(batch_result)
    
    return {'results': results, 'total_processed': len(results)}
```

### 3. å¹¶è¡Œå¤„ç†

```python
@node
def data_source(input_config):
    """æ•°æ®æº"""
    return {'data_chunks': split_data(input_config)}

@node
def parallel_processor_a(data_source):
    """å¹¶è¡Œå¤„ç†å™¨A"""
    return {'result_a': process_type_a(data_source['data_chunks'])}

@node
def parallel_processor_b(data_source):
    """å¹¶è¡Œå¤„ç†å™¨B"""  
    return {'result_b': process_type_b(data_source['data_chunks'])}

@node
def combine_results(parallel_results):
    """åˆå¹¶å¹¶è¡Œç»“æœ"""
    combined = merge_results([
        parallel_results['parallel_processor_a']['result_a'],
        parallel_results['parallel_processor_b']['result_b']
    ])
    return {'final_result': combined}

# å¹¶è¡Œç®¡é“
parallel_flow = (data_source
                 .fan_out_to([parallel_processor_a, parallel_processor_b])
                 .fan_in(combine_results))
```

## ğŸ”§ è°ƒè¯•å’Œç›‘æ§

### æ·»åŠ è°ƒè¯•ä¿¡æ¯

```python
@node
def debug_node(input_data):
    """å¸¦è°ƒè¯•çš„èŠ‚ç‚¹"""
    print(f"ğŸ” å¤„ç†æ•°æ®: {input_data}")
    
    result = process_data(input_data)
    
    print(f"âœ… å¤„ç†å®Œæˆ: {len(result)} æ¡è®°å½•")
    return result
```

### é”™è¯¯å¤„ç†

```python
@node
def safe_processor(input_data):
    """å®‰å…¨çš„å¤„ç†å™¨"""
    try:
        result = risky_operation(input_data)
        return {'success': True, 'result': result}
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'input_data': input_data
        }

@node
def error_handler(safe_processor):
    """é”™è¯¯å¤„ç†"""
    if not safe_processor['success']:
        # è®°å½•é”™è¯¯å¹¶ä½¿ç”¨é»˜è®¤å€¼
        log_error(safe_processor['error'])
        return {'result': get_default_result(), 'recovered': True}
    
    return safe_processor['result']
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. èŠ‚ç‚¹è®¾è®¡åŸåˆ™

```python
# âœ… å¥½çš„è®¾è®¡ï¼šå•ä¸€èŒè´£ã€çº¯å‡½æ•°
@node
def validate_email(email):
    import re
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return {'is_valid': bool(re.match(pattern, email)), 'email': email}

# âŒ é¿å…ï¼šå¤šé‡èŒè´£ã€å‰¯ä½œç”¨
@node  
def validate_and_send_email(email, message):
    # åŒæ—¶éªŒè¯å’Œå‘é€ï¼Œè¿åå•ä¸€èŒè´£åŸåˆ™
    pass
```

### 2. ç±»å‹æç¤º

```python
from typing import Dict, Any, List

@node
def typed_processor(
    data: Dict[str, Any], 
    threshold: float = 0.5
) -> Dict[str, Any]:
    """ç±»å‹åŒ–çš„å¤„ç†å™¨"""
    filtered_items = [
        item for item in data['items'] 
        if item['score'] >= threshold
    ]
    
    return {
        'filtered_items': filtered_items,
        'original_count': len(data['items']),
        'filtered_count': len(filtered_items)
    }
```

### 3. é…ç½®ç®¡ç†

```python
import os
from dataclasses import dataclass

@dataclass
class Config:
    database_url: str = os.getenv('DATABASE_URL', 'sqlite:///default.db')
    batch_size: int = int(os.getenv('BATCH_SIZE', '100'))
    max_retries: int = int(os.getenv('MAX_RETRIES', '3'))

config = Config()

@node
def configured_processor(data, config: Config = config):
    """ä½¿ç”¨é…ç½®çš„å¤„ç†å™¨"""
    return process_with_config(data, config)
```

## ğŸš€ ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†AetherFlowçš„åŸºç¡€ç”¨æ³•ï¼å»ºè®®æ‚¨ï¼š

1. **é˜…è¯»æ¶æ„å†³ç­–æŒ‡å—** (`docs/architecture_guide.md`) - äº†è§£å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¹¶å‘æ¨¡å¼
2. **æŸ¥çœ‹æœ€ä½³å®è·µ** (`docs/best_practices.md`) - å­¦ä¹ é«˜çº§ä½¿ç”¨æ¨¡å¼
3. **è¿è¡Œæ€§èƒ½åŸºå‡†** (`tests/performance_benchmarks.py`) - äº†è§£æ€§èƒ½ç‰¹å¾
4. **æŸ¥çœ‹ç¤ºä¾‹ä»£ç ** (`examples/`) - å­¦ä¹ å®é™…åº”ç”¨æ¡ˆä¾‹

## ğŸ¤ è·å–å¸®åŠ©

- **æ–‡æ¡£**: æŸ¥çœ‹ `docs/` ç›®å½•ä¸‹çš„è¯¦ç»†æ–‡æ¡£
- **ç¤ºä¾‹**: æŸ¥çœ‹ `examples/` ç›®å½•ä¸‹çš„å®é™…æ¡ˆä¾‹
- **æµ‹è¯•**: æŸ¥çœ‹ `tests/` ç›®å½•ä¸‹çš„æµ‹è¯•ç”¨ä¾‹

å¼€å§‹æ„å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªAetherFlowåº”ç”¨å§ï¼ ğŸš€